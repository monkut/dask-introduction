{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Dask?\n",
    "\n",
    "Dask provides multi-core and distributed parallel execution on larger-than-memory datasets.\n",
    "\n",
    "- process data that doesn't fit into memory by breaking it into blocks and specifying task chains\n",
    "- parallelize execution of tasks across cores and even nodes of a cluster\n",
    "- move computation to the data rather than the other way around, to minimize communication overheads\n",
    "\n",
    "## Prereqs\n",
    "\n",
    "This notebook assumes that you have the *Python 3.6* version of anaconda installed:\n",
    "https://www.continuum.io/downloads\n",
    "\n",
    "> Anaconda provides the majority of packages necessary for dask to run\n",
    "\n",
    "Libraries needed for dask:\n",
    "```\n",
    "# Latest dask & distributed scheduler\n",
    "python -m pip install dask[complete]\n",
    "```\n",
    "\n",
    "In addition you'll need to install `graphviz` installed to view the computation graphs generated by *dask*.\n",
    "\n",
    "```\n",
    "# install binaries on mac\n",
    "brew install graphviz\n",
    "\n",
    "# install related python library\n",
    "python -m pip install graphviz\n",
    "```\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "Dask fundamentals\n",
    "\n",
    "- Dask Task graphs\n",
    "- Dask Collections\n",
    "- Running Distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task graphs\n",
    "\n",
    "Using code very similar to standard python, dask builds up a task graph for the operations you want to perform.\n",
    "\n",
    "This task graph is then used by the schedular to determine how to parallelize the computation.\n",
    "\n",
    "This task graph can be viewed using the delayed object's, `.visualize()` method.\n",
    "Calling the `.visualize()` method does not calculate the result, only allows you to verify the computed task graph.\n",
    "\n",
    "> If graphviz is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why delay?\n",
    "\n",
    "Delayed operations allow dask to specify *how* a collection of operations should be executed. \n",
    "This *calculation specification* can then be split up and distributed across a cluster of cores or even machines.\n",
    "\n",
    "> This is similar how Spark or Airflow execute a DAG.\n",
    "\n",
    "## delayed functions and standard operators\n",
    "\n",
    "Supported operations include arithmetic operators, item or slice selection, attribute access and method calls - essentially anything that could be phrased as a lambda expression.\n",
    "\n",
    "> Operations which are not supported include mutation, setter methods, iteration (for) and bool (predicate).\n",
    "\n",
    "\n",
    "## Dask Task Graphs\n",
    "\n",
    "For all dask objects, the resulting calculated task graph is accessible via the `.dask` attribute on the resulting object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total object from obove\n",
    "total.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Collections\n",
    "\n",
    "Dask provides a number of objects or data types in the form of collections.\n",
    "Using these collections dask supports, functional, numpy and pandas style syntax.\n",
    "\n",
    "- Bag\n",
    "    - Simple object good for functional style programming supporting map, filter, and other pyspark style operations. \n",
    "- Dataframe\n",
    "   - Mimcs pandas dataframe with some limitations\n",
    "- array\n",
    "   - Mimics numpy arrays, allows for more dimensions than the dataframe object\n",
    "   \n",
    "## Dask task schedulers\n",
    "\n",
    "Dask includes a number of task schedulers, but in general the *distributed* scheduler is recommended for all usage, as it allows you to parallize tasks, not only across cores, but machines as well.\n",
    "\n",
    "## Dask distributed clusters\n",
    "\n",
    "![Dask Dataframe](./images/dask-distributed-architecture.png)\n",
    "\n",
    "\n",
    "A dask cluster consists of a scheduler and 1 or more workers.\n",
    "To interface with a cluster, you create a client object with the approriate connection information.\n",
    "\n",
    "By default, if connection information is not supplied to the `Client()` object, a scheduler will be spun up locally and x workers attached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Dataframe\n",
    "\n",
    "The Dataframe collection type mimics the pandas Dataframe API to provide an easy way to manipulate large datasets using the pandas API.\n",
    "\n",
    "![Dask Dataframe](./images/dask-dataframe-structure.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# taxi data available at:\n",
    "# http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "directory = os.path.abspath('./data/nyctaxi')\n",
    "taxi_data_files = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "\n",
    "# pandas makes it easy to analyze data IF it is in a single data frame\n",
    "# -- attempt to create a huge dataframe to get the sum of passengers\n",
    "dataframes = []\n",
    "for filepath in taxi_data_files:\n",
    "    df = pd.read_csv(filepath, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "    dataframes.append(df)\n",
    "    \n",
    "# make 1 big dataframe~!! (6.8GB)\n",
    "pandas_big_df = pd.concat(dataframes)\n",
    "\n",
    "result = pandas_big_df.passenger_count.sum()\n",
    "# howver, you may hit memory errors OR, wait a long time.... ~10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yes, there are workarounds for memory issues....\n",
    "\n",
    "# load each file and process sequentially... and sum externally to pandas\n",
    "passanger_total = 0\n",
    "for filepath in taxi_data_files:\n",
    "    df = pd.read_csv(f, nrows=5, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "    passenger_total += df.passenger_count.sum()\n",
    "\n",
    "print(passenger_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# you could even parallelize this a bit using a thread or processing pool\n",
    "from concurrent import futures\n",
    "\n",
    "def calculate_csv_column_sum(args):    \n",
    "    filepath, column_name = args\n",
    "    df = pd.read_csv(filepath, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "    return df.passenger_count.sum()\n",
    "\n",
    "args = [(filepath, 'passenger_count') for filepath in taxi_data_files]\n",
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    result = sum(result for result in executor.map(calculate_csv_column_sum, args))\n",
    "\n",
    "print(result)                 \n",
    "\n",
    "\n",
    "# ... but what if you want to do something more complex?\n",
    "# > Compute average trip distance grouped by passenger count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start local schedular and local workers\n",
    "from dask.distributed import Client\n",
    "\n",
    "# start client with:\n",
    "# dask-scheduler from the command line\n",
    "client = Client(ENTER_IPPORT_HERE)  # if IP:PORT not defined it will start a scheduler\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "\n",
    "directory = '/tmp/nyctaxi'\n",
    "files_for_analysis = os.path.join(directory, '*.csv')  \n",
    "\n",
    "df = dd.read_csv(files_for_analysis, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "client.persist(df)  # persist data to cluster\n",
    "\n",
    "# let's get that passenager count!\n",
    "operation = df.passenger_count.sum()\n",
    "result = operation.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# .dask is where the computed Task Graph is stored\n",
    "operation.dask\n",
    "\n",
    "# .visualize() can be used to visuallze the calculated task graph.\n",
    "#... however when to big you may be be able to display without adjusting your notebook output IO settings.\n",
    "operation.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "operation.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how about something more complex\n",
    "\n",
    "# Compute average trip distance grouped by passenger count\n",
    "operation = df.groupby(df.passenger_count).trip_distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "operation.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
